{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa970c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# standard python\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "#import pathlib\n",
    "\n",
    "import os\n",
    "# plotting, especially for jupyter notebooks\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True # breaks for some endpoint labels\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() # needed for tf version 1 or it stages operations but does not do them\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "# local routines\n",
    "#from chemdataprep import load_PDBs,load_countsfromPDB,load_diametersfromPDB,find_chemnames\n",
    "#from toxmathandler import load_tscores\n",
    "\n",
    "#checkpoint_path = \"/home2/ajgreen4/Read-Across_w_GAN/Models/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "print(\"tensorflow version\",tf.__version__,\". Executing eagerly?\",tf.executing_eagerly())\n",
    "print(\"Number of GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec2b4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chemdataprep as CP\n",
    "#import toxmathandler\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7092",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qm7_data = scipy.io.loadmat('qm7.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9891e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Print the dimensions of each array\n",
    "for key in ['X', 'R', 'Z', 'T', 'P']:\n",
    "    array = qm7_data.get(key)\n",
    "    if array is not None:\n",
    "        print(f\"Dimensions of {key}: {array.shape}\")\n",
    "    else:\n",
    "        print(f\"{key} not found in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095a5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the atomic numbers array\n",
    "atomic_numbers = qm7_data['Z']\n",
    "all_atomic_numbers = atomic_numbers.flatten()\n",
    "\n",
    "# Find atomic numbers\n",
    "unique_atomic_numbers = set(all_atomic_numbers)\n",
    "unique_atomic_numbers.discard(0)  # Remove 0 if it's not a valid atomic number\n",
    "\n",
    "# Print unique atomic numbers\n",
    "print(\"atomic numbers:\", unique_atomic_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0feeb",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abc10",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def atomic_number_to_symbol(atomic_number):\n",
    "    periodic_table = {\n",
    "        1: 'H', 6: 'C', 7: 'N', 8: 'O', 16: 'S'\n",
    "    }\n",
    "    return periodic_table.get(atomic_number, 'Unknown')\n",
    "\n",
    "# Convert atomic numbers to element symbols\n",
    "unique_elements = {atomic_number_to_symbol(int(num)) for num in unique_atomic_numbers}\n",
    "\n",
    "\n",
    "Z = qm7_data['Z']\n",
    "R = qm7_data['R']\n",
    "mollist = []\n",
    "\n",
    "for mol_idx in range(Z.shape[0]):\n",
    "    molecule = []\n",
    "    for atom_idx in range(Z.shape[1]):\n",
    "        atomic_number = Z[mol_idx, atom_idx]\n",
    "        if atomic_number != 0:\n",
    "            atom_type = atomic_number_to_symbol(atomic_number)\n",
    "            # Ensure coordinates are a NumPy array\n",
    "            coordinates = np.array(R[mol_idx, atom_idx, :])\n",
    "            molecule.append((atom_type, coordinates))\n",
    "    mollist.append(molecule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5ef52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def speciesmap(atom_type):\n",
    "    atom_to_number = {'H': 1, 'C': 6, 'N': 7, 'O': 8, 'S': 16}\n",
    "    #print(atom_type)\n",
    "    return np.array([atom_to_number.get(atom_type, 0)])  # Returns 0 if atom type is not recognized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19affe",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from chemdataprep import load_qm7_data\n",
    "ws, vs, Natoms, Nviews = load_qm7_data(mollist, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45290e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from chemdataprep import chemicaldiameter\n",
    "#delist=[chemicaldiameter(x) for x in mollist]\n",
    "#labels=np.array(delist)\n",
    "T=qm7_data['T']\n",
    "T_reshaped = T.reshape((7165,))\n",
    "print(type(T_reshaped))\n",
    "T_reshaped.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2beb92",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Z = qm7_data['Z']\n",
    "R = qm7_data['R']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "Z_train, Z_test, R_train, R_test, T_train, T_test = train_test_split(Z, R,T_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "def atomic_number_to_symbol(atomic_number):\n",
    "    periodic_table = {1: 'H', 6: 'C', 7: 'N', 8: 'O', 16: 'S'}\n",
    "    return periodic_table.get(atomic_number, 'Unknown')\n",
    "def convert_to_mollist1(Z_data, R_data):\n",
    "    mollist1 = []\n",
    "    for mol_idx in range(Z_data.shape[0]):\n",
    "        molecule = []\n",
    "        for atom_idx in range(Z_data.shape[1]):\n",
    "            atomic_number = Z_data[mol_idx, atom_idx]\n",
    "            if atomic_number != 0:  # Assuming 0 means no atom present\n",
    "                atom_type = atomic_number_to_symbol(atomic_number)\n",
    "                coordinates = np.array(R_data[mol_idx, atom_idx, :])\n",
    "                molecule.append((atom_type, coordinates))\n",
    "        mollist1.append(molecule)\n",
    "    return mollist1\n",
    "\n",
    "# Convert training and testing data to mollist format\n",
    "qm7_train = convert_to_mollist1(Z_train, R_train)\n",
    "qm7_test = convert_to_mollist1(Z_test, R_test)\n",
    "qm7_labels_train = T_train\n",
    "qm7_labels_test = T_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad34f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length_of_qm7_labels_train = len(qm7_labels_train)\n",
    "print(len(qm7_labels_train))\n",
    "length_of_qm7_labels_test = len(qm7_labels_test)\n",
    "print(len(qm7_labels_test))\n",
    "len(T_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f628",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws_qm7_train, vs_qm7_train, Natoms_train, Nviews_train = load_qm7_data(qm7_train, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, verbose=1)\n",
    "qm7G_train=[ws_qm7_train, vs_qm7_train]\n",
    "ws_qm7_test, vs_qm7_test, Natoms_test, Nviews_test = load_qm7_data(qm7_test, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, verbose=1)\n",
    "qm7G_test=[ws_qm7_test, vs_qm7_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "933134",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelsG_train = qm7_labels_train\n",
    "labelsG_test = qm7_labels_test\n",
    "Ntoxicity = 1\n",
    "ws_train, vs_train = ws_qm7_train, vs_qm7_train\n",
    "ws_test, vs_test = ws_qm7_test, vs_qm7_test\n",
    "dataG_train=[ws_train,vs_train]\n",
    "dataG_test=[ws_test,vs_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29786f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **Neural Network Code**\n",
    "\n",
    "#### **constructor routines**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "685028",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generic dense NN\n",
    "def multiDense(Nin,Nout,Nhidden,widthhidden=None,kernel_regularizer=None):\n",
    "    \"\"\"Construct a basic NN with some dense layers.\n",
    "    \n",
    "    :parameter Nin: The number of inputs\n",
    "    :type Nin: int\n",
    "    :parameter Nout: The number of outputs\n",
    "    :type Nout: int\n",
    "    :parameter Nhidden: The number of hidden layers.\n",
    "    :type Nhidden: int\n",
    "    :parameter widthhidden: The width of each hidden layer.\n",
    "        If left at None, Nin + Nout will be used.\n",
    "    :parameter kernel_regularizer: the regularizer to use, such as regularizers.l2(0.001)\n",
    "    :type kernel_regularizer: tensorflow.keras.regularizers.xxx\n",
    "    :returns: The NN model\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    if widthhidden is None:\n",
    "        widthhidden = Nin + Nout\n",
    "    x = inputs = keras.Input(shape=(Nin,), name='multiDense_input')\n",
    "    if kernel_regularizer is not None:\n",
    "        print(\"Using regularization\")\n",
    "    for i in range(Nhidden):\n",
    "        x = layers.Dense(widthhidden, activation='relu', kernel_regularizer=kernel_regularizer,name='dense'+str(i))(x)\n",
    "#        x = layers.Dense(widthhidden, name='dense'+str(i))(x)\n",
    "#        x = tf.nn.leaky_relu(x, alpha=0.05)\n",
    "#    outputs = layers.Dense(Nout, activation='linear',name='multiDense_output')(x)\n",
    "    outputs = layers.Dense(Nout,name='multiDense_output')(x)\n",
    "    #outputs = tf.nn.leaky_relu(outputs, alpha=0.05)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)#, name='multiDense')\n",
    "if 1:\n",
    "    # manual check of multiDense\n",
    "    mmd = multiDense(10,4,3)\n",
    "    # used to do the weighted sum over views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2791e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parallelwrapper(Nparallel,basemodel,insteadmax=False):\n",
    "    \"\"\"Construct a model that applies a basemodel multiple times and take a weighted sum (or max) of the result.\n",
    "    \n",
    "    :parameter Nparallel: The number of times to apply in parallel\n",
    "    :type Nparallel: int\n",
    "    :parameter basemodel: a keras.Model inferred to have Nin inputs and Nout outputs.\n",
    "    :type basemodel: a keras.Model\n",
    "    :parameter insteadmax: If True, take the max of the results of the basemodel instead of the weighted sum.\n",
    "        For compatibility, the model is still constructed with weights as inputs, but it ignores them.\n",
    "    :type insteadmax: Boolean\n",
    "    :returns: model with inputs shape [(?,Nparallel),(?,Nin,Nparallel)] and outputs shape (?,Nout).\n",
    "        The first input is the scalar weights in the sum.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    Note: We could do a max over the parallel applications instead of or in addition to the weighted sum.\n",
    "    \n",
    "    \"\"\"\n",
    "    # infer shape of basemodel inputs and outputs\n",
    "    Nin =  basemodel.inputs[0].shape[1]\n",
    "    Nout =  basemodel.outputs[0].shape[1]\n",
    "    \n",
    "    # Apply basemodel Nparallel times in parallel\n",
    "    # create main input (?,Nparallel,Nin) \n",
    "    parallel_inputs = keras.Input(shape=(Nparallel,Nin), name='parallelwrapper_input0')\n",
    "    # apply base NN to each parallel slice; outputs (?,Nparallel,Nout)\n",
    "    if False:\n",
    "        # original version, stopped working at some tensorflow update\n",
    "        xb = basemodel(parallel_inputs) # worked in earlier tensorflow\n",
    "        #xb = tf.map_fn(basemodel,parallel_inputs) # another version that fails\n",
    "    else:\n",
    "        # newer version, works but makes summary and graphing cumbersome\n",
    "        # unstack in the Nparallel directio\n",
    "        parallel_inputsunstacked = tf.keras.ops.unstack(parallel_inputs, Nparallel, 1)\n",
    "        # apply base NN to each \n",
    "        xbunstacked = [basemodel(x) for x in parallel_inputsunstacked]\n",
    "        # re-stack\n",
    "        xb = tf.keras.ops.stack(xbunstacked,axis=1)\n",
    "    \n",
    "    # create input scalars for weighted sun (?,Nparallel)\n",
    "    weight_inputs = keras.Input(shape=(Nparallel,), name='parallelScalars')\n",
    "    if insteadmax:\n",
    "        # take max over the Nparallel direction to get (?,1,Nout)\n",
    "        out = layers.MaxPool1D(pool_size=Nparallel)(xb)\n",
    "        # reshape to (?,Nout)\n",
    "        out = layers.Reshape((Nout,))(out)\n",
    "    else:\n",
    "        # do a weighted sum over the Nparallel direction to get (?,Nout)\n",
    "        out = layers.Dot((-2,-1))([xb,weight_inputs])\n",
    "    \n",
    "    return keras.Model(inputs=[weight_inputs,parallel_inputs], outputs=out, name='parallelwrapper')\n",
    "if 1:\n",
    "    # manual check\n",
    "    mmd = multiDense(10,4,3)\n",
    "    mpw = parallelwrapper(5,mmd,insteadmax=0)\n",
    "    # make models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcf5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_generator(data,labels,baselayers,Nfeatures,endlayers,base_regularizer=None,end_regularizer=None):\n",
    "    \"\"\"Initialize the generator neural net.\n",
    "    \n",
    "    :returns: return generator and descrimina NN.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Option changing how results of each view are aggregated\n",
    "    insteadmax = False # Does weighted average; original design\n",
    "    #insteadmax = True # Does max instead of weighted average (for both G and D)\n",
    "\n",
    "    # G\n",
    "    # base NN\n",
    "    Gbase = multiDense(data[1].shape[2],Nfeatures,baselayers,kernel_regularizer=base_regularizer) \n",
    "    # parallel view wrapper\n",
    "    Gpw = parallelwrapper(Nviews,Gbase,insteadmax)\n",
    "    # features to toxicity\n",
    "    #Gft = multiDense(Nfeatures,labels.shape[1],endlayers,kernel_regularizer=end_regularizer)\n",
    "    #we got an error when running original line since energies are one dimnsional array so we can interpret the single dimension as having only one dimension\n",
    "    Gft = multiDense(Nfeatures,1,endlayers,kernel_regularizer=end_regularizer) \n",
    "    # string together\n",
    "    generator = keras.Model(inputs=Gpw.inputs,outputs=Gft(Gpw.outputs),name='generator')\n",
    "    # make trainable\n",
    "    generator.compile(optimizer='adam',loss='mse')\n",
    "    #generator.summary()\n",
    "    # previously did better with Nfeatures=Ntoxicity and no Gft\n",
    "\n",
    "    if 0:\n",
    "        # sanity checks that model is working\n",
    "        print(\"Sanity check:\")\n",
    "        ws,vs = data\n",
    "        gbv0call = Gbase(vs[:,0,:]).numpy()\n",
    "        gbv0predict = Gbase.predict(vs[:,0,:])\n",
    "        print(\"base: 0 ?==\", np.linalg.norm(gbv0call-gbv0predict))\n",
    "        gpwcall = Gpw([ws,vs]).numpy()\n",
    "        gpwpredict = G\n",
    "        pw.predict([ws,vs])\n",
    "        print(\"wrapper: 0 ?==\",np.linalg.norm(gpwcall-gpwpredict))\n",
    "        gencall = generator([ws,vs]).numpy()\n",
    "        genpredict = generator.predict([ws,vs])\n",
    "        print(\"whole: 0 ?==\",np.linalg.norm(gencall-genpredict))\n",
    "        \n",
    "    return generator\n",
    "baselayers = 2  # hidden layers before weighted sum\n",
    "base_reg = 0.1  # regularization for the base layers\n",
    "Nfeatures = 1  # number of outputs of weighted sum\n",
    "endlayers = 1  # hidden layers after weighted sum\n",
    "end_reg = 0.1  # regularization for the end layers\n",
    "\n",
    "if base_reg == 0:\n",
    "    base_regularizer = None\n",
    "else:\n",
    "    base_regularizer = regularizers.l2(base_reg)\n",
    "\n",
    "if end_reg == 0:\n",
    "    end_regularizer = None\n",
    "else:\n",
    "    end_regularizer = regularizers.l2(end_reg)  #???\n",
    "\n",
    "print(\"(baselayers, base_reg, Nfeatures, endlayers, end_reg) =\",\n",
    "      (baselayers, base_reg, Nfeatures, endlayers, end_reg))\n",
    "# compile model with options\n",
    "generator = init_generator(dataG_train,labelsG_train,baselayers,Nfeatures,endlayers,\n",
    "                           base_regularizer=base_regularizer,end_regularizer=end_regularizer)\n",
    "generator.compile(optimizer='adam',loss='mse')\n",
    "#generator.summary()\n",
    "#change loss to MAE for energies\n",
    "dataG_train = [np.array(dataG_train[0], dtype='float32'), np.array(dataG_train[1], dtype='float32')]\n",
    "\n",
    "# Ensure labels are numpy arrays of type float32 and have a 2D shape if required\n",
    "labelsG_train = np.array(labelsG_train, dtype='float32').reshape(-1, 1)\n",
    "dataG_test = [np.array(dataG_test[0], dtype='float32'), np.array(dataG_test[1], dtype='float32')]\n",
    "labelsG_test = np.array(labelsG_test, dtype='float32').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d667",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit\n",
    "BATCH_SIZE = 32\n",
    "if 1: \n",
    "    history = generator.fit(dataG_train,labelsG_train,\n",
    "                  epochs=500,batch_size=BATCH_SIZE,verbose=0,\n",
    "                  validation_data=(dataG_test, labelsG_test))\n",
    "    print(\"train loss=\",generator.evaluate(dataG_train,labelsG_train,verbose=0))\n",
    "    print(\"test loss=\",generator.evaluate(dataG_test,labelsG_test,verbose=0))\n",
    "#     generator.fit(dataG_train,labelsG_train,epochs=1000,batch_size=BATCH_SIZE,verbose=0)\n",
    "#    generator.save(modelpath+\"AG-model-RT.h5\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1271fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot predictions of G versus truth\n",
    "def PvT_plot(model,data,labels,legend=None,title=None,doresidual=False):\n",
    "    \"\"\"Construct a plot of the true labels (x-axis) vs the data generated by the model (y-axis).\n",
    "    \n",
    "    :parameter model: the model (e.g. NN)\n",
    "    :type model: keras.model\n",
    "    :parameter data: the data that can be input to the model\n",
    "    :type data: numpy.array\n",
    "    :parameter labels: the true outputs corresponding to the data\n",
    "    :type labels: numpy.array\n",
    "    :parameter legend: The string labels corresponding to the columns of labels \n",
    "    :type legend: None or list of str\n",
    "    :parameter title: A title for the plot\n",
    "    :type title: None or string\n",
    "    :parameter doresidual: If true, plot the residual instead\n",
    "    :type doresidual: Boolean\n",
    "    \"\"\"\n",
    "        \n",
    "    gen_lab = model.predict(data)\n",
    "    if doresidual:\n",
    "        gen_lab = gen_lab - labels\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)        \n",
    "    if legend is None:\n",
    "        for i in range(labels.shape[1]):\n",
    "            plt.plot(labels[:,i],gen_lab[:,i],'o')\n",
    "    else:\n",
    "        for i in range(labels.shape[1]):\n",
    "            # include legend\n",
    "            plt.plot(labels[:,i],gen_lab[:,i],'o', label=legend[i])\n",
    "        ax.legend(bbox_to_anchor=(1.04,1), borderaxespad=0)\n",
    "    ax.set_xlabel('True Values')\n",
    "    if doresidual:\n",
    "        ax.set_ylabel('Residual Values')\n",
    "    else:\n",
    "        ax.set_ylabel('Generated Values')\n",
    "        # reference line\n",
    "        mintrue = np.min(labels)\n",
    "        maxtrue = np.max(labels)\n",
    "        plt.plot([mintrue,maxtrue],[mintrue,maxtrue])\n",
    "    if title is None:\n",
    "        title = ''\n",
    "    if 1:\n",
    "        if not doresidual:\n",
    "            gen_lab = gen_lab - labels\n",
    "        MSE = np.square(gen_lab).mean()\n",
    "        title = title+\" Mean Squared Error=\"+str(MSE)\n",
    "        print(\"Mean Squared Error: \", MSE)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b75f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "if 1:\n",
    "    plt.figure()\n",
    "    if base_regularizer is None and end_regularizer is None:\n",
    "        plt.title('Loss (no regularization)')\n",
    "    else:\n",
    "        plt.title(f'Loss, including regularization (base_reg,end_reg)=({base_reg},{end_reg})')\n",
    "    plt.semilogy(history.history['loss'], label='train')\n",
    "    plt.semilogy(history.history['val_loss'], label='test')\n",
    "    plt.ylim((0, plt.ylim()[1]))  # Set lower ylim to 0\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e9b9e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate and print mean squared error\n",
    "#if 1:\n",
    " #   gen_lab = generator.predict(dataG_train)\n",
    "  #  MSE = np.square(np.subtract(labelsG_train, gen_lab)).mean()\n",
    "   # print(\"Train Mean Squared Error: \", MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf0e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict the values using the trained model for both training and test sets\n",
    "predicted_train = generator.predict(dataG_train)\n",
    "predicted_test = generator.predict(dataG_test)\n",
    "\n",
    "# Calculate the Mean Absolute Error for the training set\n",
    "mae_train = np.mean(np.abs(predicted_train - labelsG_train))\n",
    "\n",
    "# Calculate the Mean Absolute Error for the test set\n",
    "mae_test = np.mean(np.abs(predicted_test - labelsG_test))\n",
    "\n",
    "# Print out the MAE for both sets\n",
    "print(\"Train Mean Absolute Error: \", mae_train)\n",
    "print(\"Test Mean Absolute Error: \", mae_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doresidual = False\n",
    "legend=None\n",
    "PvT_plot(generator,dataG_train,labelsG_train,title='train',doresidual=doresidual,legend=legend)\n",
    "PvT_plot(generator,dataG_test,labelsG_test,title='test',doresidual=doresidual,legend=legend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {},
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
